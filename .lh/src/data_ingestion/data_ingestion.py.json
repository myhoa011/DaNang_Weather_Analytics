{
    "sourceFile": "src/data_ingestion/data_ingestion.py",
    "activeCommit": 0,
    "commits": [
        {
            "activePatchIndex": 1,
            "patches": [
                {
                    "date": 1733410713334,
                    "content": "Index: \n===================================================================\n--- \n+++ \n"
                },
                {
                    "date": 1733410753789,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -0,0 +1,373 @@\n+import os\n+import sys\n+import json\n+import aiohttp\n+import asyncio\n+from apscheduler.schedulers.asyncio import AsyncIOScheduler\n+import aiofiles\n+import pandas as pd\n+from filelock import FileLock\n+from dotenv import load_dotenv\n+\n+sys.path.append(\".\")\n+from src.logger import logger\n+\n+# Load environment variables\n+load_dotenv()\n+\n+class DataIngestion:\n+    def __init__(self):\n+        \"\"\"Initialize DataIngestion\"\"\"\n+        self.api_url = os.getenv('DB_API_URL')\n+        self.scheduler = AsyncIOScheduler()\n+        self.data_path = os.getenv('DATA_PATH', 'data')\n+        self.data_file = f\"{self.data_path}/weather_data.json\"\n+        self.processed_file = f\"{self.data_path}/processed_data.json\"\n+        self.session = None\n+        \n+        # Create the data directory if it does not exist\n+        os.makedirs(self.data_path, exist_ok=True)\n+        \n+        # Initialize files\n+        self._init_files()\n+\n+    def _init_files(self):\n+        if not os.path.exists(self.processed_file):\n+            with open(self.processed_file, 'w') as f:\n+                json.dump({\"last_processed_dt\": 0}, f)\n+\n+    async def load_weather_data(self):\n+        \"\"\"Load weather data from file\"\"\"\n+        lock = FileLock(f\"{self.data_file}.lock\")\n+        \n+        try:\n+            with lock:\n+                if not os.path.exists(self.data_file):\n+                    logger.warning(f\"Data file {self.data_file} not found\")\n+                    return []\n+\n+                async with aiofiles.open(self.data_file, 'r', encoding='utf-8') as f:\n+                    content = await f.read()\n+                    if not content:\n+                        logger.warning(\"Data file is empty\")\n+                        return []\n+                        \n+                    data = json.loads(content)\n+                    logger.info(f\"Loaded {len(data)} weather records\")\n+                    return data\n+\n+        except json.JSONDecodeError as e:\n+            logger.error(f\"Invalid JSON format: {e}\")\n+            return []\n+        except Exception as e:\n+            logger.error(f\"Error loading weather data: {e}\")\n+            return []\n+\n+    @staticmethod\n+    def convert_to_vietnam_time(utc_timestamp: int) -> int:\n+        VIETNAM_OFFSET = 25200  # 7 hours * 3600 seconds\n+        return utc_timestamp + VIETNAM_OFFSET\n+\n+    def handle_missing_data(self, weather_data: dict) -> dict:\n+        \"\"\"\n+        Handle missing values using median from entire dataset\n+        \"\"\"\n+        try:\n+            # Load all weather data\n+            with open(self.data_file, 'r') as f:\n+                all_data = json.load(f)\n+            \n+            # Extract all entries and convert to DataFrame\n+            all_entries = [entry.get(\"data\", [{}])[0] for entry in all_data]\n+            df_all = pd.DataFrame(all_entries)\n+            \n+            # Create DataFrame for current entry\n+            df_current = pd.DataFrame([weather_data])\n+\n+            # Define columns and their types\n+            column_types = {\n+                \"temp\": \"float64\",\n+                \"pressure\": \"int64\",\n+                \"humidity\": \"int64\",\n+                \"clouds\": \"int64\",\n+                \"visibility\": \"int64\",\n+                \"wind_speed\": \"float64\",\n+                \"wind_deg\": \"int64\"\n+            }\n+\n+            # Calculate medians from all data and fill missing values\n+            for col, dtype in column_types.items():\n+                if col in df_all.columns:\n+                    # Convert all data to numeric for median calculation\n+                    df_all[col] = pd.to_numeric(df_all[col], errors='coerce')\n+                    # Calculate median from all data\n+                    median_value = df_all[col].median()\n+                    \n+                    # Convert and fill current entry\n+                    if col in df_current.columns:\n+                        df_current[col] = pd.to_numeric(df_current[col], errors='coerce')\n+                        df_current[col] = df_current[col].fillna(median_value)\n+                        \n+                        # Convert to correct type\n+                        if dtype == \"int64\":\n+                            df_current[col] = df_current[col].round().astype(dtype)\n+                        else:\n+                            df_current[col] = df_current[col].astype(dtype)\n+\n+            logger.debug(f\"Processed entry with medians from full dataset\")\n+            return df_current.iloc[0].to_dict()\n+\n+        except Exception as e:\n+            logger.error(f\"Error handling missing data: {e}\")\n+            logger.exception(\"Full traceback:\")\n+            return weather_data\n+\n+    def filter_data(self, raw_data):\n+        \"\"\"Only filter necessary fields while preserving null values\"\"\"\n+        try:\n+            weather_data = raw_data.get(\"data\", [{}])[0]\n+            utc_timestamp = weather_data.get(\"dt\")\n+            vietnam_timestamp = self.convert_to_vietnam_time(utc_timestamp)\n+            \n+            filtered_data = {\n+                \"dt\": vietnam_timestamp,\n+                \"temp\": weather_data.get(\"temp\"),\n+                \"pressure\": weather_data.get(\"pressure\"),\n+                \"humidity\": weather_data.get(\"humidity\"),\n+                \"clouds\": weather_data.get(\"clouds\"),\n+                \"visibility\": weather_data.get(\"visibility\"),\n+                \"wind_speed\": weather_data.get(\"wind_speed\"),\n+                \"wind_deg\": weather_data.get(\"wind_deg\")\n+            }\n+\n+            return filtered_data\n+\n+        except Exception as e:\n+            logger.error(f\"Error filtering data: {e}\")\n+            return None\n+\n+    async def _load_processed_data(self):\n+        \"\"\"\n+        Load the last processed timestamp from file\n+        \"\"\"\n+        try:\n+            async with aiofiles.open(self.processed_file, 'r') as f:\n+                content = await f.read()\n+                return json.loads(content)\n+        except Exception as e:\n+            logger.error(f\"Error loading processed data: {e}\")\n+            return {\"last_processed_dt\": 0}\n+\n+    async def _save_processed_data(self, data: dict):\n+        \"\"\"\n+        Save the last processed timestamp to file\n+        \"\"\"\n+        try:\n+            async with aiofiles.open(self.processed_file, 'w') as f:\n+                await f.write(json.dumps(data))\n+                await f.flush()\n+        except Exception as e:\n+            logger.error(f\"Error saving processed data: {e}\")\n+            raise\n+\n+    async def send_to_api(self, raw_data_list: list, processed_data_list: list):\n+        \"\"\"Send bulk data to API\"\"\"\n+        if self.session is None:\n+            self.session = aiohttp.ClientSession()\n+\n+        try:\n+            if not self.api_url:\n+                raise ValueError(\"API URL is not set\")\n+\n+            headers = {\n+                'Content-Type': 'application/json',\n+                'Accept': 'application/json'\n+            }\n+\n+            logger.info(f\"Sending bulk data with {len(raw_data_list)} entries\")\n+\n+            async with self.session.post(\n+                f\"{self.api_url}/api/weather/bulk\",\n+                json={\n+                    \"raw_data_list\": raw_data_list,\n+                    \"processed_data_list\": processed_data_list\n+                },\n+                headers=headers\n+            ) as response:\n+                if response.status == 200:\n+                    result = await response.json()\n+                    logger.info(f\"Successfully saved {result['count']} entries\")\n+                    return True\n+                else:\n+                    error = await response.text()\n+                    logger.error(f\"Failed to send bulk data: {response.status}, error: {error}\")\n+                    return False\n+\n+        except Exception as e:\n+            logger.error(f\"Error sending bulk data to API: {e}\")\n+            return False\n+\n+    def handle_missing_data_bulk(self, weather_data_list: list) -> list:\n+        \"\"\"\n+        Handle missing values for multiple entries using median from entire dataset\n+        \n+        Args:\n+            weather_data_list (list): List of weather data entries to process\n+            \n+        Returns:\n+            list: List of processed weather data entries with missing values filled\n+        \"\"\"\n+        try:\n+            # Load all historical data for median calculation\n+            with open(self.data_file, 'r') as f:\n+                all_data = json.load(f)\n+            \n+            # Extract all entries and convert to DataFrame\n+            all_entries = [entry.get(\"data\", [{}])[0] for entry in all_data]\n+            df_all = pd.DataFrame(all_entries)\n+            \n+            # Convert current entries to DataFrame\n+            df_current = pd.DataFrame(weather_data_list)\n+\n+            # Define columns and their types\n+            column_types = {\n+                \"temp\": \"float64\",\n+                \"pressure\": \"int64\",\n+                \"humidity\": \"int64\",\n+                \"clouds\": \"int64\",\n+                \"visibility\": \"int64\",\n+                \"wind_speed\": \"float64\",\n+                \"wind_deg\": \"int64\"\n+            }\n+\n+            # Calculate medians once for all columns\n+            medians = {}\n+            for col, dtype in column_types.items():\n+                if col in df_all.columns:\n+                    df_all[col] = pd.to_numeric(df_all[col], errors='coerce')\n+                    medians[col] = df_all[col].median()\n+\n+            logger.debug(f\"Calculated medians from {len(df_all)} historical entries\")\n+\n+            # Fill missing values for all current entries at once\n+            for col, dtype in column_types.items():\n+                if col in df_current.columns:\n+                    df_current[col] = pd.to_numeric(df_current[col], errors='coerce')\n+                    df_current[col] = df_current[col].fillna(medians.get(col, 0))\n+                    \n+                    if dtype == \"int64\":\n+                        df_current[col] = df_current[col].round().astype(dtype)\n+                    else:\n+                        df_current[col] = df_current[col].astype(dtype)\n+\n+            logger.info(f\"Processed {len(df_current)} entries with missing data\")\n+            return df_current.to_dict('records')\n+\n+        except Exception as e:\n+            logger.error(f\"Error handling missing data in bulk: {e}\")\n+            logger.exception(\"Full traceback:\")\n+            return weather_data_list\n+\n+    async def ingest(self, is_initial_run: bool = False):\n+        try:\n+            weather_data = await self.load_weather_data()\n+            if not weather_data:\n+                if is_initial_run:\n+                    logger.info(\"No weather data found during initial run\")\n+                else:\n+                    logger.info(\"No weather data found during scheduled check\")\n+                return\n+\n+            # Get last processed timestamp\n+            processed_data = await self._load_processed_data()\n+            last_processed_dt = processed_data.get(\"last_processed_dt\", 0)\n+\n+            if is_initial_run:\n+                logger.info(f\"Initial run - Last processed timestamp: {last_processed_dt}\")\n+            else:\n+                logger.info(f\"Checking for data newer than: {last_processed_dt}\")\n+\n+            # Filter only new data\n+            raw_data_list = []\n+            latest_dt = last_processed_dt\n+\n+            for entry in weather_data:\n+                raw_data = self.filter_data(entry)\n+                if raw_data and raw_data[\"dt\"] > last_processed_dt:\n+                    raw_data_list.append(raw_data)\n+                    latest_dt = max(latest_dt, raw_data[\"dt\"])\n+\n+            if raw_data_list:\n+                count = len(raw_data_list)\n+                if is_initial_run:\n+                    logger.info(f\"Initial run - Found {count} entries to process\")\n+                else:\n+                    logger.info(f\"Found {count} new entries to process\")\n+\n+                processed_data_list = self.handle_missing_data_bulk(raw_data_list)\n+                success = await self.send_to_api(raw_data_list, processed_data_list)\n+                \n+                if success:\n+                    await self._save_processed_data({\"last_processed_dt\": latest_dt})\n+                    if is_initial_run:\n+                        logger.info(f\"Initial run - Successfully processed {count} entries\")\n+                    else:\n+                        logger.info(f\"Successfully processed {count} new entries\")\n+                else:\n+                    logger.error(\"Failed to send bulk data\")\n+            else:\n+                if is_initial_run:\n+                    logger.info(\"Initial run - No new data to process\")\n+                else:\n+                    logger.info(\"No new data to process\")\n+\n+        except Exception as e:\n+            if is_initial_run:\n+                logger.error(f\"Error during initial ingestion: {e}\")\n+            else:\n+                logger.error(f\"Error during scheduled ingestion: {e}\")\n+            logger.exception(\"Full traceback:\")\n+\n+    async def start(self):\n+        \"\"\"Start the Weather Data Ingestion service\"\"\"\n+        try:\n+            logger.info(\"Starting Weather Data Ingestion service\")\n+            \n+            # First run - process all existing data\n+            await self.ingest(is_initial_run=True)\n+            \n+            # Schedule future runs\n+            self.scheduler.add_job(\n+                self.ingest,\n+                'interval',\n+                minutes=1,\n+                id='weather_data_ingestion'\n+            )\n+            \n+            self.scheduler.start()\n+\n+            try:\n+                while True:\n+                    await asyncio.sleep(1)\n+            except KeyboardInterrupt:\n+                await self.stop()\n+\n+        except Exception as e:\n+            logger.error(f\"Error starting service: {e}\")\n+            await self.stop()\n+\n+    async def stop(self):\n+        try:\n+            self.scheduler.shutdown()\n+            if self.session:\n+                await self.session.close()\n+            logger.info(\"Weather Data Ingestion service stopped\")\n+        except Exception as e:\n+            logger.error(f\"Error stopping service: {e}\")\n+\n+async def main():\n+    service = DataIngestion()\n+    await service.start()\n+\n+if __name__ == \"__main__\":\n+    asyncio.run(main()) \n\\ No newline at end of file\n"
                }
            ],
            "date": 1733410713334,
            "name": "Commit-0",
            "content": "import os\nimport sys\nimport json\nimport time\nfrom datetime import datetime\nimport aiohttp\nimport asyncio\nfrom apscheduler.schedulers.asyncio import AsyncIOScheduler\nimport aiofiles\nimport numpy as np\nimport pandas as pd\nfrom dotenv import load_dotenv\n\nsys.path.append(\".\")\nfrom src.logger import logger\n\n# Load environment variables\nload_dotenv()\n\nclass DataIngestion:\n    def __init__(self, api_url):\n        \"\"\"Initialize DataIngestion with Database API URL\"\"\"\n        self.api_url = api_url\n        self.scheduler = AsyncIOScheduler()\n        self.data_path = os.getenv('DATA_PATH', 'data')\n        self.data_file = f\"{self.data_path}/weather_data.json\"\n        self.processed_file = f\"{self.data_path}/processed_data.json\"\n        self.session = None\n        \n        # Create the data directory if it does not exist\n        os.makedirs(self.data_path, exist_ok=True)\n        \n        # Initialize files\n        self._init_files()\n\n    def _init_files(self):\n        if not os.path.exists(self.processed_file):\n            with open(self.processed_file, 'w') as f:\n                json.dump({\"last_processed_dt\": 0}, f)\n\n    async def load_weather_data(self):\n        if not os.path.exists(self.data_file):\n            return None\n        try:\n            async with aiofiles.open(self.data_file, \"r\") as file:\n                content = await file.read()\n                data = json.loads(content)\n                return data if data else None\n                \n        except Exception as e:\n            logger.error(f\"Error loading weather data: {e}\")\n            return None\n\n    @staticmethod\n    def convert_to_vietnam_time(utc_timestamp: int) -> int:\n        VIETNAM_OFFSET = 25200  # 7 hours * 3600 seconds\n        return utc_timestamp + VIETNAM_OFFSET\n\n    def handle_missing_data(self, weather_data: dict) -> dict:\n        \"\"\"\n        Handle missing values using median from entire dataset\n        \"\"\"\n        try:\n            # Load all weather data\n            with open(self.data_file, 'r') as f:\n                all_data = json.load(f)\n            \n            # Extract all entries and convert to DataFrame\n            all_entries = [entry.get(\"data\", [{}])[0] for entry in all_data]\n            df_all = pd.DataFrame(all_entries)\n            \n            # Create DataFrame for current entry\n            df_current = pd.DataFrame([weather_data])\n\n            # Define columns and their types\n            column_types = {\n                \"temp\": \"float64\",\n                \"pressure\": \"int64\",\n                \"humidity\": \"int64\",\n                \"clouds\": \"int64\",\n                \"visibility\": \"int64\",\n                \"wind_speed\": \"float64\",\n                \"wind_deg\": \"int64\"\n            }\n\n            # Calculate medians from all data and fill missing values\n            for col, dtype in column_types.items():\n                if col in df_all.columns:\n                    # Convert all data to numeric for median calculation\n                    df_all[col] = pd.to_numeric(df_all[col], errors='coerce')\n                    # Calculate median from all data\n                    median_value = df_all[col].median()\n                    \n                    # Convert and fill current entry\n                    if col in df_current.columns:\n                        df_current[col] = pd.to_numeric(df_current[col], errors='coerce')\n                        df_current[col] = df_current[col].fillna(median_value)\n                        \n                        # Convert to correct type\n                        if dtype == \"int64\":\n                            df_current[col] = df_current[col].round().astype(dtype)\n                        else:\n                            df_current[col] = df_current[col].astype(dtype)\n\n            logger.debug(f\"Processed entry with medians from full dataset\")\n            return df_current.iloc[0].to_dict()\n\n        except Exception as e:\n            logger.error(f\"Error handling missing data: {e}\")\n            logger.exception(\"Full traceback:\")\n            return weather_data\n\n    def filter_data(self, raw_data):\n        \"\"\"Only filter necessary fields while preserving null values\"\"\"\n        try:\n            weather_data = raw_data.get(\"data\", [{}])[0]\n            utc_timestamp = weather_data.get(\"dt\")\n            vietnam_timestamp = self.convert_to_vietnam_time(utc_timestamp)\n            \n            filtered_data = {\n                \"dt\": vietnam_timestamp,\n                \"temp\": weather_data.get(\"temp\"),\n                \"pressure\": weather_data.get(\"pressure\"),\n                \"humidity\": weather_data.get(\"humidity\"),\n                \"clouds\": weather_data.get(\"clouds\"),\n                \"visibility\": weather_data.get(\"visibility\"),\n                \"wind_speed\": weather_data.get(\"wind_speed\"),\n                \"wind_deg\": weather_data.get(\"wind_deg\")\n            }\n\n            return filtered_data\n\n        except Exception as e:\n            logger.error(f\"Error filtering data: {e}\")\n            return None\n\n    async def _load_processed_data(self):\n        \"\"\"\n        Load the last processed timestamp from file\n        \"\"\"\n        try:\n            async with aiofiles.open(self.processed_file, 'r') as f:\n                content = await f.read()\n                return json.loads(content)\n        except Exception as e:\n            logger.error(f\"Error loading processed data: {e}\")\n            return {\"last_processed_dt\": 0}\n\n    async def _save_processed_data(self, data: dict):\n        \"\"\"\n        Save the last processed timestamp to file\n        \"\"\"\n        try:\n            async with aiofiles.open(self.processed_file, 'w') as f:\n                await f.write(json.dumps(data))\n                await f.flush()\n        except Exception as e:\n            logger.error(f\"Error saving processed data: {e}\")\n            raise\n\n    async def send_to_api(self, raw_data_list: list, processed_data_list: list):\n        if self.session is None:\n            self.session = aiohttp.ClientSession()\n\n        try:\n            if not self.api_url:\n                raise ValueError(\"API URL is not set\")\n\n            base_url = self.api_url.rstrip('/')\n            raw_url = f\"{base_url}/api/raw_weather/bulk\"\n            processed_url = f\"{base_url}/api/process_weather/bulk\"\n\n            headers = {\n                'Content-Type': 'application/json',\n                'Accept': 'application/json'\n            }\n\n            logger.info(f\"Sending bulk data with {len(raw_data_list)} entries\")\n\n            # Send bulk data\n            raw_task = self.session.post(raw_url, json=raw_data_list, headers=headers)\n            processed_task = self.session.post(processed_url, json=processed_data_list, headers=headers)\n\n            raw_response, processed_response = await asyncio.gather(\n                raw_task, \n                processed_task,\n                return_exceptions=True\n            )\n\n            if isinstance(raw_response, Exception) or isinstance(processed_response, Exception):\n                logger.error(f\"Error in API calls: Raw: {raw_response}, Processed: {processed_response}\")\n                return False\n\n            if raw_response.status == 200:\n                raw_result = await raw_response.json()\n                logger.info(f\"Successfully saved {raw_result['count']} raw entries\")\n            else:\n                raw_error = await raw_response.text()\n                logger.error(f\"Failed to send raw data: {raw_response.status}, error: {raw_error}\")\n                return False\n\n            if processed_response.status == 200:\n                processed_result = await processed_response.json()\n                logger.info(f\"Successfully saved {processed_result['count']} processed entries\")\n            else:\n                processed_error = await processed_response.text()\n                logger.error(f\"Failed to send processed data: {processed_response.status}, error: {processed_error}\")\n                return False\n\n            return True\n\n        except Exception as e:\n            logger.error(f\"Error sending bulk data to API: {e}\")\n            return False\n\n    def handle_missing_data_bulk(self, weather_data_list: list) -> list:\n        \"\"\"\n        Handle missing values for multiple entries using median from entire dataset\n        \n        Args:\n            weather_data_list (list): List of weather data entries to process\n            \n        Returns:\n            list: List of processed weather data entries with missing values filled\n        \"\"\"\n        try:\n            # Load all historical data for median calculation\n            with open(self.data_file, 'r') as f:\n                all_data = json.load(f)\n            \n            # Extract all entries and convert to DataFrame\n            all_entries = [entry.get(\"data\", [{}])[0] for entry in all_data]\n            df_all = pd.DataFrame(all_entries)\n            \n            # Convert current entries to DataFrame\n            df_current = pd.DataFrame(weather_data_list)\n\n            # Define columns and their types\n            column_types = {\n                \"temp\": \"float64\",\n                \"pressure\": \"int64\",\n                \"humidity\": \"int64\",\n                \"clouds\": \"int64\",\n                \"visibility\": \"int64\",\n                \"wind_speed\": \"float64\",\n                \"wind_deg\": \"int64\"\n            }\n\n            # Calculate medians once for all columns\n            medians = {}\n            for col, dtype in column_types.items():\n                if col in df_all.columns:\n                    df_all[col] = pd.to_numeric(df_all[col], errors='coerce')\n                    medians[col] = df_all[col].median()\n\n            logger.debug(f\"Calculated medians from {len(df_all)} historical entries\")\n\n            # Fill missing values for all current entries at once\n            for col, dtype in column_types.items():\n                if col in df_current.columns:\n                    df_current[col] = pd.to_numeric(df_current[col], errors='coerce')\n                    df_current[col] = df_current[col].fillna(medians.get(col, 0))\n                    \n                    if dtype == \"int64\":\n                        df_current[col] = df_current[col].round().astype(dtype)\n                    else:\n                        df_current[col] = df_current[col].astype(dtype)\n\n            logger.info(f\"Processed {len(df_current)} entries with missing data\")\n            return df_current.to_dict('records')\n\n        except Exception as e:\n            logger.error(f\"Error handling missing data in bulk: {e}\")\n            logger.exception(\"Full traceback:\")\n            return weather_data_list\n\n    async def ingest(self, is_initial_run: bool = False):\n        try:\n            weather_data = await self.load_weather_data()\n            if not weather_data:\n                if is_initial_run:\n                    logger.info(\"No weather data found during initial run\")\n                else:\n                    logger.info(\"No weather data found during scheduled check\")\n                return\n\n            # Get last processed timestamp\n            processed_data = await self._load_processed_data()\n            last_processed_dt = processed_data.get(\"last_processed_dt\", 0)\n\n            if is_initial_run:\n                logger.info(f\"Initial run - Last processed timestamp: {last_processed_dt}\")\n            else:\n                logger.info(f\"Checking for data newer than: {last_processed_dt}\")\n\n            # Filter only new data\n            raw_data_list = []\n            latest_dt = last_processed_dt\n\n            for entry in weather_data:\n                raw_data = self.filter_data(entry)\n                if raw_data and raw_data[\"dt\"] > last_processed_dt:\n                    raw_data_list.append(raw_data)\n                    latest_dt = max(latest_dt, raw_data[\"dt\"])\n\n            if raw_data_list:\n                count = len(raw_data_list)\n                if is_initial_run:\n                    logger.info(f\"Initial run - Found {count} entries to process\")\n                else:\n                    logger.info(f\"Found {count} new entries to process\")\n\n                processed_data_list = self.handle_missing_data_bulk(raw_data_list)\n                success = await self.send_to_api(raw_data_list, processed_data_list)\n                \n                if success:\n                    await self._save_processed_data({\"last_processed_dt\": latest_dt})\n                    if is_initial_run:\n                        logger.info(f\"Initial run - Successfully processed {count} entries\")\n                    else:\n                        logger.info(f\"Successfully processed {count} new entries\")\n                else:\n                    logger.error(\"Failed to send bulk data\")\n            else:\n                if is_initial_run:\n                    logger.info(\"Initial run - No new data to process\")\n                else:\n                    logger.info(\"No new data to process\")\n\n        except Exception as e:\n            if is_initial_run:\n                logger.error(f\"Error during initial ingestion: {e}\")\n            else:\n                logger.error(f\"Error during scheduled ingestion: {e}\")\n            logger.exception(\"Full traceback:\")\n\n    async def start(self):\n        \"\"\"Start the Weather Data Ingestion service\"\"\"\n        try:\n            logger.info(\"Starting Weather Data Ingestion service\")\n            \n            # First run - process all existing data\n            await self.ingest(is_initial_run=True)\n            \n            # Schedule future runs\n            self.scheduler.add_job(\n                self.ingest,\n                'interval',\n                minutes=1,\n                id='weather_data_ingestion'\n            )\n            \n            self.scheduler.start()\n\n            try:\n                while True:\n                    await asyncio.sleep(1)\n            except KeyboardInterrupt:\n                await self.stop()\n\n        except Exception as e:\n            logger.error(f\"Error starting service: {e}\")\n            await self.stop()\n\n    async def stop(self):\n        try:\n            self.scheduler.shutdown()\n            if self.session:\n                await self.session.close()\n            logger.info(\"Weather Data Ingestion service stopped\")\n        except Exception as e:\n            logger.error(f\"Error stopping service: {e}\")\n\nasync def main():\n    api_url = os.getenv('DB_API_URL')\n    if not api_url:\n        logger.error(\"DB_API_URL not set in environment variables\")\n        return\n    \n    service = DataIngestion(api_url)\n    await service.start()\n\nif __name__ == \"__main__\":\n    asyncio.run(main()) "
        }
    ]
}